{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pyserini - LTR.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DtMzFWaFLBxv"
      ],
      "machine_shape": "hm",
      "background_execution": "on",
      "authorship_tag": "ABX9TyNwsPxERjFII/1kDEBPyQCT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luanps/pyserini/blob/master/Pyserini_LTR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LTR - Learning-To-Rank Reranking Models for MS MARCO Passage\n"
      ],
      "metadata": {
        "id": "ekFmONZUGNnl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Dependencies"
      ],
      "metadata": {
        "id": "DtMzFWaFLBxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "mtJGBBXeCkhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#%%capture\n",
        "!curl -O https://download.java.net/java/GA/jdk11/9/GPL/openjdk-11.0.2_linux-x64_bin.tar.gz\n",
        "!mv openjdk-11.0.2_linux-x64_bin.tar.gz /usr/lib/jvm/; cd /usr/lib/jvm/; tar -zxvf openjdk-11.0.2_linux-x64_bin.tar.gz\n",
        "!update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk-11.0.2/bin/java 1\n",
        "!update-alternatives --set java /usr/lib/jvm/jdk-11.0.2/bin/java\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/jdk-11.0.2\""
      ],
      "metadata": {
        "id": "hPntWabOHsVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp gs://luanps/information_retrieval/pyserini/bm25/run.msmarco-passage.bm25tuned.txt ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAN3H8gdCvzG",
        "outputId": "c85d7aa6-ff5d-4b8a-b622-d95b5e139275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://luanps/information_retrieval/pyserini/bm25/run.msmarco-passage.bm25tuned.txt...\n",
            "| [1 files][126.9 MiB/126.9 MiB]                                                \n",
            "Operation completed over 1 objects/126.9 MiB.                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5bCNTymGI2E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0054fc8-aa85-45e8-9fa3-dc1aa8e5d703"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pyserini'...\n",
            "remote: Enumerating objects: 5240, done.\u001b[K\n",
            "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 5240 (delta 34), reused 26 (delta 13), pack-reused 5157\u001b[K\n",
            "Receiving objects: 100% (5240/5240), 2.42 MiB | 4.45 MiB/s, done.\n",
            "Resolving deltas: 100% (3667/3667), done.\n",
            "Submodule 'tools' (https://github.com/castorini/anserini-tools.git) registered for path 'tools'\n",
            "Cloning into '/content/pyserini/tools'...\n",
            "remote: Enumerating objects: 353, done.        \n",
            "remote: Counting objects: 100% (110/110), done.        \n",
            "remote: Compressing objects: 100% (84/84), done.        \n",
            "remote: Total 353 (delta 54), reused 63 (delta 25), pack-reused 243        \n",
            "Receiving objects: 100% (353/353), 23.60 MiB | 12.35 MiB/s, done.\n",
            "Resolving deltas: 100% (138/138), done.\n",
            "Submodule path 'tools': checked out '63ceeab1dd94c1221f29b931d868e8fab67cc25c'\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/luanps/pyserini.git  --recurse-submodules"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyserini "
      ],
      "metadata": {
        "id": "ciph9Vw4Ka5L",
        "outputId": "4779f5ea-fa22-4543-ecbe-3f2f0a2cf273",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyserini\n",
            "  Downloading pyserini-0.16.0-py3-none-any.whl (84.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 84.6 MB 194 kB/s \n",
            "\u001b[?25hCollecting nmslib>=2.1.1\n",
            "  Downloading nmslib-2.1.1-cp37-cp37m-manylinux2010_x86_64.whl (13.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.5 MB 26.0 MB/s \n",
            "\u001b[?25hCollecting lightgbm>=3.3.2\n",
            "  Downloading lightgbm-3.3.2-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 22.4 MB/s \n",
            "\u001b[?25hCollecting sentencepiece>=0.1.95\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 60.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.7/dist-packages (from pyserini) (1.21.5)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from pyserini) (1.0.2)\n",
            "Requirement already satisfied: Cython>=0.29.21 in /usr/local/lib/python3.7/dist-packages (from pyserini) (0.29.28)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from pyserini) (1.3.5)\n",
            "Collecting transformers>=4.6.0\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 49.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pyserini) (4.63.0)\n",
            "Collecting onnxruntime>=1.8.1\n",
            "  Downloading onnxruntime-1.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 78.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from pyserini) (1.4.1)\n",
            "Collecting spacy>=3.2.1\n",
            "  Downloading spacy-3.2.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 5.0 MB/s \n",
            "\u001b[?25hCollecting pyjnius>=1.4.0\n",
            "  Downloading pyjnius-1.4.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 20.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm>=3.3.2->pyserini) (0.37.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from nmslib>=2.1.1->pyserini) (5.4.8)\n",
            "Collecting pybind11<2.6.2\n",
            "  Downloading pybind11-2.6.1-py2.py3-none-any.whl (188 kB)\n",
            "\u001b[K     |████████████████████████████████| 188 kB 77.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime>=1.8.1->pyserini) (3.17.3)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime>=1.8.1->pyserini) (2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->pyserini) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->pyserini) (2018.9)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from pyjnius>=1.4.0->pyserini) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->pyserini) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->pyserini) (1.1.0)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (1.0.6)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (3.10.0.2)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (0.9.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (3.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (57.4.0)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 25.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (2.0.6)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 64.6 MB/s \n",
            "\u001b[?25hCollecting thinc<8.1.0,>=8.0.12\n",
            "  Downloading thinc-8.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n",
            "\u001b[K     |████████████████████████████████| 628 kB 76.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.1->pyserini) (21.3)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 74.0 MB/s \n",
            "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.2.1->pyserini) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=3.2.1->pyserini) (3.0.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=3.2.1->pyserini) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (1.24.3)\n",
            "Collecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 51.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->pyserini) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->pyserini) (3.6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.6.0->pyserini) (4.11.2)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 75.5 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 62.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy>=3.2.1->pyserini) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=3.2.1->pyserini) (2.0.1)\n",
            "Installing collected packages: catalogue, typer, srsly, pyyaml, pydantic, tokenizers, thinc, spacy-loggers, spacy-legacy, sacremoses, pybind11, pathy, langcodes, huggingface-hub, transformers, spacy, sentencepiece, pyjnius, onnxruntime, nmslib, lightgbm, pyserini\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "  Attempting uninstall: lightgbm\n",
            "    Found existing installation: lightgbm 2.2.3\n",
            "    Uninstalling lightgbm-2.2.3:\n",
            "      Successfully uninstalled lightgbm-2.2.3\n",
            "Successfully installed catalogue-2.0.6 huggingface-hub-0.4.0 langcodes-3.3.0 lightgbm-3.3.2 nmslib-2.1.1 onnxruntime-1.10.0 pathy-0.6.1 pybind11-2.6.1 pydantic-1.8.2 pyjnius-1.4.1 pyserini-0.16.0 pyyaml-6.0 sacremoses-0.0.47 sentencepiece-0.1.96 spacy-3.2.3 spacy-legacy-3.0.9 spacy-loggers-1.0.1 srsly-2.4.2 thinc-8.0.13 tokenizers-0.11.6 transformers-4.17.0 typer-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "27uC8sG7HL5h",
        "outputId": "acf42515-32e9-44e0-9f83-395982a51c1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-22.0.3-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-60.9.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 53.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.37.1)\n",
            "Installing collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed pip-22.0.3 setuptools-60.9.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.2.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.0.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.13)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.63.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (60.9.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting en-core-web-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.2.0) (3.2.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.63.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (60.9.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.2.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "7pc5TNOHUfR3",
        "outputId": "82eaffef-7134-456c-a5e9-1c3903865b95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''!wget https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/pyserini-models/model-ltr-ibm.tar.gz -P collections/msmarco-ltr-passage/\n",
        "!tar -xzvf collections/msmarco-ltr-passage/model-ltr-ibm.tar.gz -C collections/msmarco-ltr-passage/\n",
        "\n",
        "!wget https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/pyserini-models/model-ltr-msmarco-passage-mrr-v1.tar.gz -P runs/\n",
        "!tar -xzvf runs/model-ltr-msmarco-passage-mrr-v1.tar.gz -C runs'''"
      ],
      "metadata": {
        "id": "dq20sKdOLZY7",
        "outputId": "57d3ba5f-ae5b-47ea-a001-56a71e916bf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!wget https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/pyserini-models/model-ltr-ibm.tar.gz -P collections/msmarco-ltr-passage/\\n!tar -xzvf collections/msmarco-ltr-passage/model-ltr-ibm.tar.gz -C collections/msmarco-ltr-passage/\\n\\n!wget https://rgw.cs.uwaterloo.ca/JIMMYLIN-bucket0/pyserini-models/model-ltr-msmarco-passage-mrr-v1.tar.gz -P runs/\\n!tar -xzvf runs/model-ltr-msmarco-passage-mrr-v1.tar.gz -C runs'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://msmarco.blob.core.windows.net/msmarcoranking/qidpidtriples.train.full.2.tsv.gz -P collections/msmarco-passage/\t\n",
        "!gzip -d collections/msmarco-passage/qidpidtriples.train.full.2.tsv.gz"
      ],
      "metadata": {
        "id": "ceW1TYX3zTQL",
        "outputId": "77189236-6d9c-40d7-cae7-7ef1d2f9ceb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-06 14:57:53--  https://msmarco.blob.core.windows.net/msmarcoranking/qidpidtriples.train.full.2.tsv.gz\n",
            "Resolving msmarco.blob.core.windows.net (msmarco.blob.core.windows.net)... 20.150.34.4\n",
            "Connecting to msmarco.blob.core.windows.net (msmarco.blob.core.windows.net)|20.150.34.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1841693309 (1.7G) [application/x-gzip]\n",
            "Saving to: ‘collections/msmarco-passage/qidpidtriples.train.full.2.tsv.gz’\n",
            "\n",
            "qidpidtriples.train 100%[===================>]   1.71G  8.68MB/s    in 3m 14s  \n",
            "\n",
            "2022-03-06 15:01:08 (9.05 MB/s) - ‘collections/msmarco-passage/qidpidtriples.train.full.2.tsv.gz’ saved [1841693309/1841693309]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Download MSMARCO dataset\n",
        "\n",
        "!mkdir collections/msmarco-passage\n",
        "!wget https://msmarco.blob.core.windows.net/msmarcoranking/collectionandqueries.tar.gz -P collections/msmarco-passage\n",
        "# Alternative mirror:\n",
        "# wget https://www.dropbox.com/s/9f54jg2f71ray3b/collectionandqueries.tar.gz -P collections/msmarco-passage\n",
        "!tar xvfz collections/msmarco-passage/collectionandqueries.tar.gz -C collections/msmarco-passage"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPcArEOjK8ug",
        "outputId": "1c068db1-bf3e-43a2-f083-c07a6da79a2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘collections/msmarco-passage’: File exists\n",
            "--2022-03-06 15:02:13--  https://msmarco.blob.core.windows.net/msmarcoranking/collectionandqueries.tar.gz\n",
            "Resolving msmarco.blob.core.windows.net (msmarco.blob.core.windows.net)... 20.150.34.4\n",
            "Connecting to msmarco.blob.core.windows.net (msmarco.blob.core.windows.net)|20.150.34.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1057717952 (1009M) [application/gzip]\n",
            "Saving to: ‘collections/msmarco-passage/collectionandqueries.tar.gz’\n",
            "\n",
            "collectionandquerie 100%[===================>]   1009M  5.80MB/s    in 2m 46s  \n",
            "\n",
            "2022-03-06 15:05:00 (6.08 MB/s) - ‘collections/msmarco-passage/collectionandqueries.tar.gz’ saved [1057717952/1057717952]\n",
            "\n",
            "collection.tsv\n",
            "qrels.dev.small.tsv\n",
            "qrels.train.tsv\n",
            "queries.dev.small.tsv\n",
            "queries.dev.tsv\n",
            "queries.eval.small.tsv\n",
            "queries.eval.tsv\n",
            "queries.train.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r pyserini/tools/ .\n",
        "!cp -r pyserini/scripts ."
      ],
      "metadata": {
        "id": "qNfh4xz71Muk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''!mkdir collections/msmarco-ltr-passage/\n",
        "\n",
        "!python pyserini/scripts/ltr_msmarco/convert_queries.py \\\n",
        "  --input collections/msmarco-passage/queries.eval.small.tsv \\\n",
        "  --output collections/msmarco-ltr-passage/queries.eval.small.json \n",
        "\n",
        "!python pyserini/scripts/ltr_msmarco/convert_queries.py \\\n",
        "  --input collections/msmarco-passage/queries.dev.small.tsv \\\n",
        "  --output collections/msmarco-ltr-passage/queries.dev.small.json\n",
        "\n",
        "!python pyserini/scripts/ltr_msmarco/convert_queries.py \\\n",
        "  --input collections/msmarco-passage/queries.train.tsv \\\n",
        "  --output collections/msmarco-ltr-passage/queries.train.json\n",
        "\n",
        "#!gsutil cp -r collections/msmarco-ltr-passage/  gs://luanps/information_retrieval/pyserini/ltr/'''"
      ],
      "metadata": {
        "id": "uuliIi8t_U0z",
        "outputId": "b21eb310-116b-40a6-eda0-ca4e0f41729b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!mkdir collections/msmarco-ltr-passage/\\n\\n!python pyserini/scripts/ltr_msmarco/convert_queries.py   --input collections/msmarco-passage/queries.eval.small.tsv   --output collections/msmarco-ltr-passage/queries.eval.small.json \\n\\n!python pyserini/scripts/ltr_msmarco/convert_queries.py   --input collections/msmarco-passage/queries.dev.small.tsv   --output collections/msmarco-ltr-passage/queries.dev.small.json\\n\\n!python pyserini/scripts/ltr_msmarco/convert_queries.py   --input collections/msmarco-passage/queries.train.tsv   --output collections/msmarco-ltr-passage/queries.train.json\\n\\n#!gsutil cp -r collections/msmarco-ltr-passage/  gs://luanps/information_retrieval/pyserini/ltr/'"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp -r   gs://luanps/information_retrieval/pyserini/ltr/msmarco-ltr-passage/ collections/"
      ],
      "metadata": {
        "id": "hlT7smPeACev",
        "outputId": "c576587c-3d14-43fe-d4a6-9ea2579a29c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://luanps/information_retrieval/pyserini/ltr/msmarco-ltr-passage/ibm_model/body/output.t1.5.bin...\n",
            "Copying gs://luanps/information_retrieval/pyserini/ltr/msmarco-ltr-passage/ibm_model/body/source.vcb...\n",
            "Copying gs://luanps/information_retrieval/pyserini/ltr/msmarco-ltr-passage/ibm_model/body/target.vcb...\n",
            "Copying gs://luanps/information_retrieval/pyserini/ltr/msmarco-ltr-passage/ibm_model/text_bert_tok/output.t1.5.bin...\n",
            "-\n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying gs://luanps/information_retrieval/pyserini/ltr/msmarco-ltr-passage/ibm_model/text_bert_tok/source.vcb...\n",
            "Copying gs://luanps/information_retrieval/pyserini/ltr/msmarco-ltr-passage/ibm_model/text_bert_tok/target.vcb...\n",
            "Copying gs://luanps/information_retrieval/pyserini/ltr/msmarco-ltr-passage/ibm_model/title_unlemm/output.t1.5.bin...\n",
            "Copying gs://luanps/information_retrieval/pyserini/ltr/msmarco-ltr-passage/ibm_model/title_unlemm/source.vcb...\n",
            "Copying gs://luanps/information_retrieval/pyserini/ltr/msmarco-ltr-passage/ibm_model/title_unlemm/target.vcb...\n",
            "Copying gs://luanps/information_retrieval/pyserini/ltr/msmarco-ltr-passage/ibm_model/url_unlemm/output.t1.5.bin...\n",
            "Copying gs://luanps/information_retrieval/pyserini/ltr/msmarco-ltr-passage/ibm_model/url_unlemm/source.vcb...\n",
            "Copying gs://luanps/information_retrieval/pyserini/ltr/msmarco-ltr-passage/ibm_model/url_unlemm/target.vcb...\n",
            "Copying gs://luanps/information_retrieval/pyserini/ltr/msmarco-ltr-passage/model-ltr-ibm.tar.gz...\n",
            "Copying gs://luanps/information_retrieval/pyserini/ltr/msmarco-ltr-passage/queries.dev.small.json...\n",
            "Copying gs://luanps/information_retrieval/pyserini/ltr/msmarco-ltr-passage/queries.eval.small.json...\n",
            "Copying gs://luanps/information_retrieval/pyserini/ltr/msmarco-ltr-passage/queries.train.json...\n",
            "/ [16 files][905.8 MiB/905.8 MiB]   50.4 MiB/s                                  \n",
            "Operation completed over 16 objects/905.8 MiB.                                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run LTR inference"
      ],
      "metadata": {
        "id": "gP6TIgN9LBIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pyserini.search.lucene.ltr  --input run.msmarco-passage.bm25tuned.txt \\\n",
        "  --input-format tsv \\\n",
        "  --model runs/msmarco-passage-ltr-mrr-v1 \\\n",
        "  --index msmarco-passage-ltr \\\n",
        "  --data passage \\\n",
        "  --ibm-model collections/msmarco-ltr-passage/ibm_model/ \\\n",
        "  --queries collections/msmarco-ltr-passage \\\n",
        "--output run.ltr.msmarco-passage.tsv "
      ],
      "metadata": {
        "id": "g6PNQJnsKlKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MRR Eval\n",
        "!python -m pyserini.eval.msmarco_passage_eval msmarco-passage-dev-subset run.ltr.msmarco-passage.tsv >> ltr_mrr_eval.txt\n",
        "\n"
      ],
      "metadata": {
        "id": "ZmbIG56-LwKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TREC Eval\n",
        "!python -m pyserini.eval.convert_msmarco_run_to_trec_run --input run.ltr.msmarco-passage.tsv --output run.ltr.msmarco-passage.trec\n",
        "\n",
        "!python -m pyserini.eval.trec_eval -c -mrecall.1000 \\\n",
        "                                      -mmap msmarco-passage-dev-subset \\\n",
        "                                      run.ltr.msmarco-passage.trec >> ltr_trec_eval.txt"
      ],
      "metadata": {
        "id": "FON0q1NsVQuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pyserini.eval.convert_msmarco_run_to_trec_run \\\n",
        "   \n",
        "\n",
        "!python -m pyserini/tools/scripts/msmarco/convert_msmarco_to_trec_qrels \\\n",
        "   --input tools/topics-and-qrels/qrels.msmarco-passage.dev-subset.txt --output collections/msmarco-passage/qrels.dev.small.trec"
      ],
      "metadata": {
        "id": "U-CnmlrXU50s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "$ tools/eval/trec_eval.9.0.4/trec_eval -c -mrecall.1000 -mmap \\\n",
        "   collections/msmarco-passage/qrels.dev.small.trec runs/run.ltr.msmarco-passage.trec\n",
        "map                   \tall\t0.2551\n",
        "recall_1000           \tall\t0.8573       \t"
      ],
      "metadata": {
        "id": "55mjIA6TVBY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LTR Feature Extraction"
      ],
      "metadata": {
        "id": "q2RVHxtcodVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pyserini.search.lucene.ltr  --input run.msmarco-passage.bm25tuned.txt \\\n",
        "  --input-format tsv \\\n",
        "  --model runs/msmarco-passage-ltr-mrr-v1 \\\n",
        "  --index msmarco-passage-ltr \\\n",
        "  --data passage \\\n",
        "  --ibm-model collections/msmarco-ltr-passage/ibm_model/ \\\n",
        "  --queries collections/msmarco-ltr-passage \\\n",
        "--output run.ltr.msmarco-passage.tsv "
      ],
      "metadata": {
        "id": "RURD4nj_okVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# We're going to explicitly use a local installation of Pyserini (as opposed to a pip-installed one).\n",
        "# Comment these lines out to use a pip-installed one instead.\n",
        "sys.path.insert(0, './')\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import multiprocessing\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "#from pyserini.search.lucene.ltr._search_msmarco import MsmarcoLtrSearcher\n",
        "#from pyserini.search.lucene.ltr import *\n",
        "\n",
        "\n",
        "def train_data_loader(task='triple', neg_sample=20, random_seed=12345):\n",
        "    print(f'train_{task}_sampled_with_{neg_sample}_{random_seed}.pickle')\n",
        "    if os.path.exists(f'./collections/msmarco-ltr-passage/train_{task}_sampled_with_{neg_sample}_{random_seed}.pickle'):\n",
        "        sampled_train = pd.read_pickle(f'./collections/msmarco-ltr-passage/train_{task}_sampled_with_{neg_sample}_{random_seed}.pickle')\n",
        "        print(sampled_train.shape)\n",
        "        print(sampled_train.index.get_level_values('qid').drop_duplicates().shape)\n",
        "        print(sampled_train.groupby('qid').count().mean())\n",
        "        print(sampled_train.head(10))\n",
        "        print(sampled_train.info())\n",
        "        return sampled_train\n",
        "    else:\n",
        "        if task == 'triple':\n",
        "            train = pd.read_csv('./collections/msmarco-passage/qidpidtriples.train.full.2.tsv', sep=\"\\t\",\n",
        "                                names=['qid', 'pos_pid', 'neg_pid'], dtype=str)#np.int32)\n",
        "            pos_half = train[['qid', 'pos_pid']].rename(columns={\"pos_pid\": \"pid\"}).drop_duplicates()\n",
        "            pos_half['rel'] = np.int32(1)\n",
        "            neg_half = train[['qid', 'neg_pid']].rename(columns={\"neg_pid\": \"pid\"}).drop_duplicates()\n",
        "            neg_half['rel'] = np.int32(0)\n",
        "            del train\n",
        "            sampled_neg_half = []\n",
        "            for qid, group in tqdm(neg_half.groupby('qid')):\n",
        "                sampled_neg_half.append(group.sample(n=min(neg_sample, len(group)), random_state=random_seed))\n",
        "            sampled_train = pd.concat([pos_half] + sampled_neg_half, axis=0, ignore_index=True)\n",
        "            sampled_train = sampled_train.sort_values(['qid', 'pid']).set_index(['qid', 'pid'])\n",
        "            print(sampled_train.shape)\n",
        "            print(sampled_train.index.get_level_values('qid').drop_duplicates().shape)\n",
        "            print(sampled_train.groupby('qid').count().mean())\n",
        "            print(sampled_train.head(10))\n",
        "            print(sampled_train.info())\n",
        "\n",
        "            sampled_train.to_pickle(f'./collections/msmarco-ltr-passage/train_{task}_sampled_with_{neg_sample}_{random_seed}.pickle')\n",
        "        elif task == 'rank':\n",
        "            qrel = defaultdict(list)\n",
        "            with open(\"./collections/msmarco-passage/qrels.train.tsv\") as f:\n",
        "                for line in f:\n",
        "                    topicid, _, docid, rel = line.strip().split('\\t')\n",
        "                    assert rel == \"1\", line.split(' ')\n",
        "                    qrel[topicid].append(docid)\n",
        "\n",
        "            qid2pos = defaultdict(list)\n",
        "            qid2neg = defaultdict(list)\n",
        "            with open(\"./runs/msmarco-passage/run.train.small.tsv\") as f:\n",
        "                for line in tqdm(f):\n",
        "                    topicid, docid, rank = line.split()\n",
        "                    assert topicid in qrel\n",
        "                    if docid in qrel[topicid]:\n",
        "                        qid2pos[topicid].append(docid)\n",
        "                    else:\n",
        "                        qid2neg[topicid].append(docid)\n",
        "            sampled_train = []\n",
        "            for topicid, pos_list in tqdm(qid2pos.items()):\n",
        "                neg_list = random.sample(qid2neg[topicid], min(len(qid2neg[topicid]), neg_sample))\n",
        "                for positive_docid in pos_list:\n",
        "                    sampled_train.append((int(topicid), int(positive_docid), 1))\n",
        "                for negative_docid in neg_list:\n",
        "                    sampled_train.append((int(topicid), int(negative_docid), 0))\n",
        "            sampled_train = pd.DataFrame(sampled_train, columns=['qid', 'pid', 'rel'], dtype=np.int32)\n",
        "            sampled_train = sampled_train.sort_values(['qid', 'pid']).set_index(['qid', 'pid'])\n",
        "            print(sampled_train.shape)\n",
        "            print(sampled_train.index.get_level_values('qid').drop_duplicates().shape)\n",
        "            print(sampled_train.groupby('qid').count().mean())\n",
        "            print(sampled_train.head(10))\n",
        "            print(sampled_train.info())\n",
        "\n",
        "            sampled_train.to_pickle(f'./collections/msmarco-ltr-passage/train_{task}_sampled_with_{neg_sample}_{random_seed}.pickle')\n",
        "        else:\n",
        "            raise Exception('unknown parameters')\n",
        "        return sampled_train\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Running prediction on candidates\n",
        "\"\"\"\n",
        "def dev_data_loader(file, format, data, top=100):\n",
        "    if format == 'tsv':\n",
        "        dev = pd.read_csv(file, sep=\"\\t\",\n",
        "                          names=['qid', 'pid', 'rank'],\n",
        "                          dtype={'qid': 'S','pid': 'S', 'rank':'i',})\n",
        "    elif format == 'trec':\n",
        "        dev = pd.read_csv(file, sep=\"\\s+\",\n",
        "                    names=['qid', 'q0', 'pid', 'rank', 'score', 'tag'],\n",
        "                    usecols=['qid', 'pid', 'rank'],\n",
        "                    dtype={'qid': 'S','pid': 'S', 'rank':'i',})\n",
        "    else:\n",
        "        raise Exception('unknown parameters')\n",
        "    assert dev['qid'].dtype == object\n",
        "    assert dev['pid'].dtype == object\n",
        "    assert dev['rank'].dtype == np.int32\n",
        "    dev = dev[dev['rank']<=top]\n",
        "    if data == 'passage':\n",
        "        dev_qrel = pd.read_csv('tools/topics-and-qrels/qrels.msmarco-passage.dev-subset.txt', sep=\" \",\n",
        "                            names=[\"qid\", \"q0\", \"pid\", \"rel\"], usecols=['qid', 'pid', 'rel'],\n",
        "                            dtype={'qid': 'S','pid': 'S', 'rel':'i'})\n",
        "    elif data == 'document':\n",
        "        dev_qrel = pd.read_csv('tools/topics-and-qrels/qrels.msmarco-doc.dev.txt', sep=\"\\t\",\n",
        "                            names=[\"qid\", \"q0\", \"pid\", \"rel\"], usecols=['qid', 'pid', 'rel'],\n",
        "                            dtype={'qid': 'S','pid': 'S', 'rel':'i'})\n",
        "    assert dev['qid'].dtype == object\n",
        "    assert dev['pid'].dtype == object\n",
        "    assert dev['rank'].dtype == np.int32\n",
        "    dev = dev.merge(dev_qrel, left_on=['qid', 'pid'], right_on=['qid', 'pid'], how='left')\n",
        "    dev['rel'] = dev['rel'].fillna(0).astype(np.int32)\n",
        "    dev = dev.sort_values(['qid', 'pid']).set_index(['qid', 'pid'])\n",
        "\n",
        "    print(dev.shape)\n",
        "    print(dev.index.get_level_values('qid').drop_duplicates().shape)\n",
        "    print(dev.groupby('qid').count().mean())\n",
        "    print(dev.head(10))\n",
        "    print(dev.info())\n",
        "\n",
        "    dev_rel_num = dev_qrel[dev_qrel['rel'] > 0].groupby('qid').count()['rel']\n",
        "\n",
        "    recall_point = [10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000]\n",
        "    recall_curve = {k: [] for k in recall_point}\n",
        "    for qid, group in tqdm(dev.groupby('qid')):\n",
        "        group = group.reset_index()\n",
        "        assert len(group['pid'].tolist()) == len(set(group['pid'].tolist()))\n",
        "        total_rel = dev_rel_num.loc[qid]\n",
        "        query_recall = [0 for k in recall_point]\n",
        "        for t in group.sort_values('rank').itertuples():\n",
        "            if t.rel > 0:\n",
        "                for i, p in enumerate(recall_point):\n",
        "                    if t.rank <= p:\n",
        "                        query_recall[i] += 1\n",
        "        for i, p in enumerate(recall_point):\n",
        "            if total_rel > 0:\n",
        "                recall_curve[p].append(query_recall[i] / total_rel)\n",
        "            else:\n",
        "                recall_curve[p].append(0.)\n",
        "\n",
        "    for k, v in recall_curve.items():\n",
        "        avg = np.mean(v)\n",
        "        print(f'recall@{k}:{avg}')\n",
        "\n",
        "    return dev, dev_qrel\n",
        "\n",
        "\n",
        "def query_loader():\n",
        "    queries = {}\n",
        "    with open(f\"{args['queries']}\") as f:\n",
        "        for line in f:\n",
        "            query = json.loads(line)\n",
        "            qid = query.pop('id')\n",
        "            query['analyzed'] = query['analyzed'].split(\" \")\n",
        "            query['text'] = query['text_unlemm'].split(\" \")\n",
        "            query['text_unlemm'] = query['text_unlemm'].split(\" \")\n",
        "            query['text_bert_tok'] = query['text_bert_tok'].split(\" \")\n",
        "            queries[qid] = query\n",
        "    return queries\n",
        "\n",
        "\n",
        "def eval_mrr(dev_data):\n",
        "    score_tie_counter = 0\n",
        "    score_tie_query = set()\n",
        "    MRR = []\n",
        "    for qid, group in tqdm(dev_data.groupby('qid')):\n",
        "        group = group.reset_index()\n",
        "        rank = 0\n",
        "        prev_score = None\n",
        "        assert len(group['pid'].tolist()) == len(set(group['pid'].tolist()))\n",
        "        # stable sort is also used in LightGBM\n",
        "\n",
        "        for t in group.sort_values('score', ascending=False, kind='mergesort').itertuples():\n",
        "            if prev_score is not None and abs(t.score - prev_score) < 1e-8:\n",
        "                score_tie_counter += 1\n",
        "                score_tie_query.add(qid)\n",
        "            prev_score = t.score\n",
        "            rank += 1\n",
        "            if t.rel > 0:\n",
        "                MRR.append(1.0 / rank)\n",
        "                break\n",
        "            elif rank == 10 or rank == len(group):\n",
        "                MRR.append(0.)\n",
        "                break\n",
        "\n",
        "    score_tie = f'score_tie occurs {score_tie_counter} times in {len(score_tie_query)} queries'\n",
        "    print(score_tie)\n",
        "    mrr_10 = np.mean(MRR).item()\n",
        "    print(f'MRR@10:{mrr_10} with {len(MRR)} queries')\n",
        "    return {'score_tie': score_tie, 'mrr_10': mrr_10}\n",
        "\n",
        "\n",
        "def eval_recall(dev_qrel, dev_data):\n",
        "    dev_rel_num = dev_qrel[dev_qrel['rel'] > 0].groupby('qid').count()['rel']\n",
        "\n",
        "    score_tie_counter = 0\n",
        "    score_tie_query = set()\n",
        "\n",
        "    recall_point = [10,20,50,100,200,250,300,333,400,500,1000]\n",
        "    recall_curve = {k: [] for k in recall_point}\n",
        "    for qid, group in tqdm(dev_data.groupby('qid')):\n",
        "        group = group.reset_index()\n",
        "        rank = 0\n",
        "        prev_score = None\n",
        "        assert len(group['pid'].tolist()) == len(set(group['pid'].tolist()))\n",
        "        # stable sort is also used in LightGBM\n",
        "        total_rel = dev_rel_num.loc[qid]\n",
        "        query_recall = [0 for k in recall_point]\n",
        "        for t in group.sort_values('score', ascending=False, kind='mergesort').itertuples():\n",
        "            if prev_score is not None and abs(t.score - prev_score) < 1e-8:\n",
        "                score_tie_counter += 1\n",
        "                score_tie_query.add(qid)\n",
        "            prev_score = t.score\n",
        "            rank += 1\n",
        "            if t.rel > 0:\n",
        "                for i, p in enumerate(recall_point):\n",
        "                    if rank <= p:\n",
        "                        query_recall[i] += 1\n",
        "        for i, p in enumerate(recall_point):\n",
        "            if total_rel > 0:\n",
        "                recall_curve[p].append(query_recall[i] / total_rel)\n",
        "            else:\n",
        "                recall_curve[p].append(0.)\n",
        "\n",
        "    score_tie = f'score_tie occurs {score_tie_counter} times in {len(score_tie_query)} queries'\n",
        "    print(score_tie)\n",
        "    res = {'score_tie': score_tie}\n",
        "\n",
        "    for k, v in recall_curve.items():\n",
        "        avg = np.mean(v)\n",
        "        print(f'recall@{k}:{avg}')\n",
        "        res[f'recall@{k}'] = avg\n",
        "\n",
        "    return res"
      ],
      "metadata": {
        "id": "AVdbspMtooBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    'input': 'run.msmarco-passage.bm25tuned.txt',\n",
        "    'input_format': 'tsv',\n",
        "    'model': 'runs/msmarco-passage-ltr-mrr-v1',\n",
        "    'index': 'msmarco-passage-ltr',\n",
        "    'data': 'passage',\n",
        "    'ibm_model': 'collections/msmarco-ltr-passage/ibm_model/',\n",
        "    'queries':  'collections/msmarco-ltr-passage/queries.train.json',\n",
        "    'output': 'run.ltr.msmarco-passage.tsv',\n",
        "    'reranking_top': 10000,\n",
        "    'max_passage': True,\n",
        "    'neg_sample': 10\n",
        "  }"
      ],
      "metadata": {
        "id": "tipGP3e55F21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "#sampled_train = train_data_loader(task='triple', neg_sample = args['neg_sample'])\n",
        "#!gsutil cp collections/msmarco-ltr-passage/train_triple_sampled_with_10_12345.pickle gs://luanps/information_retrieval/pyserini/ltr/\n",
        "!gsutil cp   gs://luanps/information_retrieval/pyserini/ltr/train_triple_sampled_with_10_12345.pickle  collections/msmarco-ltr-passage/\n",
        "sampled_train = pickle.load(open('collections/msmarco-ltr-passage/train_triple_sampled_with_10_12345.pickle','rb'))\n",
        "\n",
        "sampled_train = sampled_train.reset_index()\n",
        "sampled_train['qid'] = sampled_train['qid'].astype(str)\n",
        "sampled_train['pid'] = sampled_train['pid'].astype(str)"
      ],
      "metadata": {
        "id": "Me2dbkRCzTGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = query_loader()"
      ],
      "metadata": {
        "id": "C842bk7j7FjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dev, dev_qrel = dev_data_loader(args['input'], args['input_format'], args['data'], args['reranking_top'])"
      ],
      "metadata": {
        "id": "HZoHFa5E6FnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyserini.search.lucene.ltr import MsmarcoLtrSearcher\n",
        "\n",
        "searcher = MsmarcoLtrSearcher(args['model'], args['ibm_model'], args['index'], args['data'])\n",
        "searcher.add_fe()"
      ],
      "metadata": {
        "id": "nk7zcXN1_V63",
        "outputId": "1c2f12c7-409e-4a8a-e07b-6a6febb4906e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to initialize pre-built index msmarco-passage-ltr.\n",
            "/root/.cache/pyserini/indexes/index-msmarco-passage-ltr-20210519-e25e33f.a5de642c268ac1ed5892c069bdc29ae3 already exists, skipping download.\n",
            "Initializing msmarco-passage-ltr...\n",
            "Attempting to initialize pre-built index msmarco-passage-ltr.\n",
            "/root/.cache/pyserini/indexes/index-msmarco-passage-ltr-20210519-e25e33f.a5de642c268ac1ed5892c069bdc29ae3 already exists, skipping download.\n",
            "Initializing msmarco-passage-ltr...\n",
            "analyzed contents\n",
            "text_unlemm text_unlemm\n",
            "text_bert_tok text_bert_tok\n",
            "IBM model Load takes 11.49 seconds\n",
            "IBM model Load takes 24.33 seconds\n",
            "IBM model Load takes 234.83 seconds\n",
            "IBM model Load takes 43.38 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_extract(searcher, df, queries, fe):\n",
        "    tasks = []\n",
        "    task_infos = []\n",
        "    group_lst = []\n",
        "\n",
        "    for qid, group in tqdm(df.groupby('qid')):\n",
        "        '''try:\n",
        "            query_dict = queries[str(qid)]\n",
        "\n",
        "        except:\n",
        "            continue'''\n",
        "            \n",
        "        task = {\n",
        "            \"qid\": qid,\n",
        "            \"docIds\": [],\n",
        "            \"rels\": [],\n",
        "            \"query_dict\": queries[str(qid)]\n",
        "        }\n",
        "        for t in group.reset_index().itertuples():\n",
        "            if (searcher.data == 'document'):\n",
        "                if (searcher.index_reader.doc(t.pid) != None):\n",
        "                    task[\"docIds\"].append(t.pid)\n",
        "                    task_infos.append((qid, t.pid, t.rel))\n",
        "            else:\n",
        "                task[\"docIds\"].append(t.pid)\n",
        "                task_infos.append((qid, t.pid, t.rel))\n",
        "        tasks.append(task)\n",
        "        group_lst.append((qid, len(task['docIds'])))\n",
        "\n",
        "\n",
        "        if len(tasks) == 1000:\n",
        "            features = fe.batch_extract(tasks)\n",
        "            task_infos = pd.DataFrame(task_infos, columns=['qid', 'pid', 'rel'])\n",
        "            group = pd.DataFrame(group_lst, columns=['qid', 'count'])\n",
        "            print(features.shape)\n",
        "            print(task_infos.qid.drop_duplicates().shape)\n",
        "            print(group.mean())\n",
        "            print(features.head(10))\n",
        "            print(features.info())\n",
        "            yield task_infos, features, group\n",
        "            tasks = []\n",
        "            task_infos = []\n",
        "            group_lst = []\n",
        "    # deal with rest\n",
        "    if len(tasks) > 0:\n",
        "        features = fe.batch_extract(tasks)\n",
        "        task_infos = pd.DataFrame(task_infos, columns=['qid', 'pid', 'rel'])\n",
        "        group = pd.DataFrame(group_lst, columns=['qid', 'count'])\n",
        "        print(features.shape)\n",
        "        print(task_infos.qid.drop_duplicates().shape)\n",
        "        print(group.mean())\n",
        "        print(features.head(10))\n",
        "        print(features.info())\n",
        "        yield task_infos, features, group\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "wT382jwaz0V-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_info = []\n",
        "for extracted in batch_extract(searcher, sampled_train, queries, searcher.fe): #searcher.batch_extract(sampled_train, queries, searcher.fe): #\n",
        "    batch_info.append(extracted)\n",
        "pickle.dump(batch_info,open('msmarco_passage_train.ltr.features.pkl','wb'))\n",
        "!gsutil cp msmarco_passage_train.ltr.features.pkl gs://luanps/information_retrieval/pyserini/ltr/"
      ],
      "metadata": {
        "id": "NiAIKzYJAkZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save train data"
      ],
      "metadata": {
        "id": "QpYkGqT8m1XE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp gs://luanps/information_retrieval/pyserini/ltr/train_triple_sampled_with_10_12345.pickle ."
      ],
      "metadata": {
        "id": "YLo1nGk-m5BZ",
        "outputId": "5e46ad81-8113-49b6-ca12-cffef1c0a90d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://luanps/information_retrieval/pyserini/ltr/train_triple_sampled_with_10_12345.pickle...\n",
            "/ [1 files][ 76.9 MiB/ 76.9 MiB]                                                \n",
            "Operation completed over 1 objects/76.9 MiB.                                     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n2 collections/msmarco-passage/queries.train.tsv"
      ],
      "metadata": {
        "id": "JX4GPDqAqC8z",
        "outputId": "2a96fcac-b4c3-4473-e149-1abe039d77ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "121352\tdefine extreme\n",
            "634306\twhat does chattel mean on credit history\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle \n",
        "train_triple = pickle.load(open('train_triple_sampled_with_10_12345.pickle','rb'))\n",
        "train_triple = train_triple.reset_index()"
      ],
      "metadata": {
        "id": "LZO3K2jom6f3"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "collection = pd.read_csv('./collections/msmarco-passage/collection.tsv', sep=\"\\t\",\n",
        "                          names=['pid', 'passage'],\n",
        "                          dtype={'pid': 'i', 'passage':'S',})\n",
        "\n",
        "queries = pd.read_csv('./collections/msmarco-passage/queries.train.tsv', sep=\"\\t\",\n",
        "                          names=['qid', 'query'],\n",
        "                          dtype={'qid': 'i', 'query':'S',})"
      ],
      "metadata": {
        "id": "_n9-fNLfnBd1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_query = train_triple.merge(queries, on='qid')\n",
        "train_collection = train_query.merge(collection, on='pid')"
      ],
      "metadata": {
        "id": "fh6ULCIFqhdX"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(train_collection,open('msmarco_passage_train.ltr.collection.pkl','wb'))\n",
        "!gsutil cp msmarco_passage_train.ltr.collection.pkl gs://luanps/information_retrieval/pyserini/ltr/"
      ],
      "metadata": {
        "id": "ceqiuspKq7d5",
        "outputId": "40b30554-c699-44e9-9765-e4957d9687ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file://msmarco_passage_train.ltr.collection.pkl [Content-Type=application/octet-stream]...\n",
            "/ [0 files][    0.0 B/  1.2 GiB]                                                \r==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "\\\n",
            "Operation completed over 1 objects/1.2 GiB.                                      \n"
          ]
        }
      ]
    }
  ]
}